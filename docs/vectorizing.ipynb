{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start\n",
    "To compare text we have to make vector of features out of them. First clean it up and prepare your text.\n",
    "\n",
    "# Cleaning\n",
    "\n",
    "Given corpus of sentences let's prepare them before vectorizing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is my life, beautiful life',\n",
    "    'Life is hard and full of problems',\n",
    "    'Red fox ran after blue cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stopwords\n",
    "Stopwords are just stop-words, there are no features at all. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life, beautiful life', 'life hard full problems', 'red fox ran blue cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def text_without_stopwords(input):\n",
    "    without = [z for z in input.lower().split() if z not in stop]\n",
    "    return ' '.join(without)\n",
    "\n",
    "corpus = [text_without_stopwords(sentence) for sentence in corpus]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize - punctuation and stemming\n",
    "There are some words that are equal as far as meaning but has different endings. Moreover we don't want to have commas and dots included in vectorization. So let's stem and remove punctuation using `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life beauti life', 'life hard full problem', 'red fox ran blue cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "remove_punctuation_map = dict((ord(char), None) for char in punctuation)\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "def normalize(text):\n",
    "    return stem_tokens(word_tokenize(text.translate(remove_punctuation_map)))\n",
    "corpus = [' '.join(normalize(sentence)) for sentence in corpus]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "Now we have words cleaned and prepared. Let's map word->number, this vectors will be later on processed as regular number vectors. One of the ways is to use TfIdf - it takes term frequency (how often word occurs in document) and inverted document frequency (how rare is the word throughout documents) to produce final vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beauti', 'blue', 'cat', 'fox', 'full', 'hard', 'life', 'problem', 'ran', 'red']\n",
      "  (0, 6)\t0.835591541945\n",
      "  (0, 0)\t0.549351231026\n",
      "  (1, 7)\t0.52863460666\n",
      "  (1, 6)\t0.402040244161\n",
      "  (1, 5)\t0.52863460666\n",
      "  (1, 4)\t0.52863460666\n",
      "  (2, 9)\t0.4472135955\n",
      "  (2, 8)\t0.4472135955\n",
      "  (2, 3)\t0.4472135955\n",
      "  (2, 2)\t0.4472135955\n",
      "  (2, 1)\t0.4472135955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "docs_fit = vectorizer.fit(corpus)\n",
    "features = docs_fit.get_feature_names()\n",
    "tdifd = docs_fit.transform(corpus)\n",
    "print(features)\n",
    "print(tdifd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the highest \"notes\" has `life` word in first document. It occurs there twice. But in the 2nd document it has lower score (0.40) comparing to other once-occui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
